% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/reddit.R
\name{reddit}
\alias{reddit}
\alias{reddit.karma}
\alias{reddit.usercomments}
\alias{reddit.lsm}
\title{Reddit}
\usage{
reddit(topics, search = NULL, sort = "hot", filename = "reddit.csv",
  write = TRUE, lim = 100, filter = "\\\\[removed\\\\]", clean = TRUE,
  ...)

reddit.karma(users)

reddit.usercomments(users, filename = NULL, subreddits = NULL, lim = 100,
  type = "comments", useragent = paste("R LUSI @", date()))

reddit.lsm(data)
}
\arguments{
\item{topics}{A string or vector of strings corresponding to subreddit names (e.g., 'trees'
referring to reddit.com/r/trees/). Only the first value is used if search is specified.}

\item{search}{Passed to \code{\link[RedditExtractoR]{find_thread_urls}} as the search_terms argument.
If this is specified, if topic is specified, the first topic value will be added as the
subreddit argument (which will restrict search to that subreddit).}

\item{sort}{How to sort initial comments. Only applies if search is not specified. Default is
'hot', with 'new', 'rising', 'top', 'gilded', and 'ads', as options.}

\item{filename}{Name of the file to be saved in the current working directory. This will
currently always be a csv file.}

\item{write}{Logical: if FALSE, data will not be save to a file (they will just be stored as
objects if you've named your reddit call).}

\item{lim}{Numeric: sets the number of posts to pull per topic. The max is 100 if search is not
specified. If search is specified, lim is ignored.}

\item{filter}{Passed to \code{\link{grepl}}. A pattern used to filter posts by the content of
their comments. default is '\[removed\]' to filter out those comments that have been deleted.}

\item{clean}{Logical; if \code{FALSE}, converts curly to straight quotes.}

\item{...}{Passed additional arguments to \code{\link[RedditExtractoR]{find_thread_urls}} if search
is specified.}

\item{users}{A vector of user names (as in reddit.com/user/username; such as those in the user
column from the reddit function). Information is never gathered twice for the same user;
\code{\link{reddit.usercomments}} simply removes duplicate user names (i.e., unique(users)),
whereas reddit.karma will return karma scores in the same order as the input, filling in the
same information for duplicate users.}

\item{subreddits}{A vector of subreddits to filter for; only comments within the specified
subreddits are returned. Should exactly match the subreddit_name_prefix field (e.g., 'r/Anxiety'
for \href{reddit.com/r/Anxiety}{https://www.reddit.com/r/Anxiety/}), including 'r/' before each
subreddit name, though this will be added if missing.}

\item{type}{Type of user data to download; either 'comments' or 'submissions' (posts).}

\item{useragent}{String to set as the request's User-Agent.}

\item{data}{The data frame returned from a reddit call (e.g.,
\code{comments = reddit('trees'); reddit.lsm(comments)})}
}
\description{
Essentially a wrapper for the RedditExtractoR package; pulls comments from specified subreddits
or searches.
}
\examples{
\dontrun{
# these will all save a file called 'reddit.csv' to the current working directory.
# pulls from a single, depression related subreddit:
reddit("depression")

# pull from a few subreddits, also saving the data as an object ('comments'):
topics <- c("trees", "Meditation")
comments <- reddit(topics)

# pull comments from a search
reddit(search = "politics")

# calculate language style matching between each comment and the comment it's replying to
# within the first thread of the trees subreddit
thread_lsm <- reddit.lsm(comments[comments$title == comments$title[1], ])

# download the 5 most recent comments from 10 users who commented in the trees subreddit
user_comments <- reddit.usercomments(comments$user[1:5], lim = 5)
}
}
